{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0aee2f",
   "metadata": {},
   "source": [
    "# Feature engineering & Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83849ae2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e95bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c15c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('bpi2017_test.csv')\n",
    "df_train = pd.read_csv(\"bpi2017_train.csv\")\n",
    "df_val = pd.read_csv(\"bpi2017_val.csv\")\n",
    "\n",
    "df_test['time:timestamp'] = pd.to_datetime(df_test['time:timestamp'])\n",
    "df_train['time:timestamp'] = pd.to_datetime(df_train['time:timestamp'])\n",
    "df_val['time:timestamp'] = pd.to_datetime(df_val['time:timestamp'])\n",
    "\n",
    "df_train = df_train.drop(columns=['Unnamed: 0'])\n",
    "df_val = df_val.drop(columns=[\"Unnamed: 0\"])\n",
    "df_test = df_test.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047c5bb",
   "metadata": {},
   "source": [
    "## Global features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d16c4e",
   "metadata": {},
   "source": [
    "### Case occurrence number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"case_occurrence_no\"] = df_train.groupby(['case:concept:name'])['time:timestamp'].cumcount().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3d63e",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cols = ['EventOrigin', 'Action', 'lifecycle:transition']\n",
    "df_train = pd.get_dummies(df_train, columns=encoded_cols, prefix=[\"EventOrigin_is\", \"action_is\", 'lifecycle:transition_is'])\n",
    "df_val = pd.get_dummies(df_val, columns=encoded_cols, prefix=[\"EventOrigin_is\", \"action_is\", 'lifecycle:transition_is'])\n",
    "df_test = pd.get_dummies(df_test, columns=encoded_cols, prefix=[\"EventOrigin_is\", \"action_is\", 'lifecycle:transition_is'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30c307",
   "metadata": {},
   "source": [
    "# Creating additional features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a826089",
   "metadata": {},
   "source": [
    "### Next and past activity timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_past_activity(df):\n",
    "    temp = df['time:timestamp']\n",
    "    next_activity = []\n",
    "    for i in range(len(temp)-1):\n",
    "        next_activity.append(temp[i+1])\n",
    "\n",
    "    df['next_activity_delta_t'] = pd.Series(next_activity) - df['time:timestamp']\n",
    "    df['past_activity_delta_t'] = df['time:timestamp'] - pd.Series(next_activity)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0592dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative sum function to be used later\n",
    "def CumSum(lists):\n",
    "    # Returns the cumulative sum of a list\n",
    "    length = len(lists)\n",
    "    cu_list = [sum(lists[0: x: 1]) for x in range(0, length + 1)]\n",
    "    return cu_list[1: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_event(df):\n",
    "    # Find the next activity name by shifting the current event label\n",
    "    df['next:concept:name'] = df['concept:name'].shift(-1)\n",
    "    last_lst = [i - 1 for i in df[df['position'] == 1].index if i != 0]\n",
    "    # The next event label is 'Nothing' when the cycle is ended\n",
    "    df.at[df.shape[0] - 1, 'next:concept:name'] = 'Nothing'\n",
    "    for i in last_lst:\n",
    "        df.at[i, 'next:concept:name'] = 'Nothing'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcd02f3",
   "metadata": {},
   "source": [
    "### Time difference feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5770091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_difference(df):\n",
    "    # Calculate time difference between each row\n",
    "    df['time_diff'] = df['time:timestamp'].diff().dt.total_seconds()\n",
    "    # Set the time difference of the 1st row to 0 as it's currently NaN\n",
    "    df.at[0, 'time_diff'] = 0\n",
    "    # Count number of steps per process\n",
    "    length_per_case_List = df.groupby(['case:concept:name'])['time_diff'].count().tolist()\n",
    "\n",
    "    # Using the cumulative sum we get all the positions that are a first step in a process\n",
    "    # And then the time difference can be set to 0\n",
    "    position_lst = CumSum(length_per_case_List)\n",
    "    for i in tqdm(position_lst):\n",
    "        df.at[i, 'time_diff'] = 0\n",
    "    # For Loop mysteriously creates an empty row at the end of the df, gotta delete it\n",
    "    df = df.iloc[: -1]\n",
    "\n",
    "    # Unzip the position list to get the number of each steps of each process, make that into a list\n",
    "    step_in_process = []\n",
    "    for x in tqdm(length_per_case_List):\n",
    "        for y in range(x):\n",
    "            step_in_process.append(y + 1)\n",
    "    # Assign position number to each row/process\n",
    "    df['position'] = step_in_process\n",
    "\n",
    "    # Find future time difference by shifting the current time difference\n",
    "    df['future_time_diff'] = df['time_diff'].shift(-1)\n",
    "    df.at[df.shape[0] - 1, 'future_time_diff'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278891eb",
   "metadata": {},
   "source": [
    "### Weekday feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weekday(df):\n",
    "    # Get day of week like Monday, Tuesday, etc\n",
    "    df_day = pd.DataFrame(data = df['time:timestamp'].dt.dayofweek)\n",
    "    df_day.rename(columns = {'time:timestamp': 'day'}, inplace = True)\n",
    "    df['day'] = df_day['day']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e39eb1",
   "metadata": {},
   "source": [
    "### Working hour feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d48228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_working_hour(df):\n",
    "    # Get hour like 10, 15, etc\n",
    "    df_day = pd.DataFrame(data = df['time:timestamp'].dt.hour)\n",
    "    df_day.rename(columns = {'time:timestamp': 'hour'}, inplace = True)\n",
    "    df['hour'] = df_day['hour']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85567bf1",
   "metadata": {},
   "source": [
    "### Timestamp parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_timestamp(df):\n",
    "    temp = df[\"time:timestamp\"]\n",
    "    day_of_month = []\n",
    "    month_no = []\n",
    "    quarters = []\n",
    "    week = []\n",
    "    hour = []\n",
    "    seconds = []\n",
    "\n",
    "    for i in range(len(temp)):\n",
    "        day_of_month.append(temp[i].day)\n",
    "        month_no.append(temp[i].month)\n",
    "        quarters.append(temp[i].quarter)\n",
    "        week.append(temp[i].week)\n",
    "        hour.append(temp[i].hour)\n",
    "        seconds.append(temp[i].second)\n",
    "\n",
    "    df['day_of_month'] = pd.Series(day_of_month)\n",
    "    df['month_no'] = pd.Series(month_no)\n",
    "    df['quarter'] = pd.Series(quarters)\n",
    "    df['week'] = pd.Series(week)\n",
    "    df['hour'] = pd.Series(hour)\n",
    "    df['second'] = pd.Series(seconds)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ddd38",
   "metadata": {},
   "source": [
    "### Time difference normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a1947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_delta_t(df):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "\n",
    "    df['norm_next_activity_delta'] = min_max_scaler.fit_transform(np.array(df[\"next_activity_delta_t\"]).reshape(-1,1))\n",
    "    df['norm_past_activity_delta'] = min_max_scaler.fit_transform(np.array(df[\"past_activity_delta_t\"]).reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a17a8",
   "metadata": {},
   "source": [
    "# Applying functions on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50733841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = time_difference(df_train)\n",
    "df_val = time_difference(df_val)\n",
    "df_test = time_difference(df_test)\n",
    "\n",
    "df_train = parse_timestamp(df_train)\n",
    "df_val = parse_timestamp(df_val)\n",
    "df_test = parse_timestamp(df_test)\n",
    "\n",
    "df_train = next_past_activity(df_train)\n",
    "df_val = next_past_activity(df_val)\n",
    "df_test = next_past_activity(df_test)\n",
    "\n",
    "df_train = normalize_delta_t(df_train)\n",
    "df_val = normalize_delta_t(df_val)\n",
    "df_test = normalize_delta_t(df_test)\n",
    "\n",
    "df_train = next_event(df_train)\n",
    "df_val = next_event(df_val)\n",
    "df_test = next_event(df_test)\n",
    "\n",
    "df_train = add_weekday(df_train)\n",
    "df_val = add_weekday(df_val)\n",
    "df_test = add_weekday(df_test)\n",
    "\n",
    "df_train = add_working_hour(df_train)\n",
    "df_val = add_working_hour(df_val)\n",
    "df_test = add_working_hour(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27433f",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abf1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed_num = df_train[['case:RequestedAmount']]\n",
    "X_train_processed_cat = df_train[['action_is_Created',\n",
    "       'action_is_Deleted', 'action_is_Obtained', 'action_is_Released',\n",
    "       'action_is_statechange', 'lifecycle:transition_is_ate_abort',\n",
    "       'lifecycle:transition_is_complete', 'lifecycle:transition_is_resume',\n",
    "       'lifecycle:transition_is_schedule', 'lifecycle:transition_is_start',\n",
    "       'lifecycle:transition_is_suspend', 'lifecycle:transition_is_withdraw', 'concept:name', 'EventOrigin_is_Application',\n",
    "       'EventOrigin_is_Offer', 'EventOrigin_is_Workflow', 'case:LoanGoal', 'case:ApplicationType']]\n",
    "y_train_1 = df_train[['time:timestamp']]\n",
    "y_train_2 = df_train[['concept:name']]\n",
    "\n",
    "# One-hot encoding on categorical data\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\n",
    "transformed = enc.fit_transform(X_train_processed_cat)\n",
    "X_train_processed_cat = pd.DataFrame(transformed, columns = enc.get_feature_names())\n",
    "X_train_processed = pd.concat([X_train_processed_cat, X_train_processed_num], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71136301",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed_num = df_train[['case:RequestedAmount']]\n",
    "X_train_processed_cat = df_train[['action_is_Created',\n",
    "       'action_is_Deleted', 'action_is_Obtained', 'action_is_Released',\n",
    "       'action_is_statechange', 'lifecycle:transition_is_ate_abort',\n",
    "       'lifecycle:transition_is_complete', 'lifecycle:transition_is_resume',\n",
    "       'lifecycle:transition_is_schedule', 'lifecycle:transition_is_start',\n",
    "       'lifecycle:transition_is_suspend', 'lifecycle:transition_is_withdraw', 'concept:name', 'EventOrigin_is_Application',\n",
    "       'EventOrigin_is_Offer', 'EventOrigin_is_Workflow', 'case:LoanGoal', 'case:ApplicationType']]\n",
    "y_train_1 = df_train[['future_time_diff']]\n",
    "y_train_2 = df_train[['next:concept:name']]\n",
    "\n",
    "# One-hot encoding on categorical data\n",
    "enc = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\n",
    "transformed = enc.fit_transform(X_train_processed_cat)\n",
    "X_train_processed_cat = pd.DataFrame(transformed, columns = enc.get_feature_names())\n",
    "X_train_processed = pd.concat([X_train_processed_cat, X_train_processed_num], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd771794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the score for each variable for time prediction\n",
    "skb_time = SelectKBest(score_func = f_regression)\n",
    "skb_time.fit_transform(X_train_processed, y_train_1)\n",
    "score_dct_time = dict(zip(X_train_processed.columns.tolist(), skb_time.scores_.round(decimals = 1).tolist()))\n",
    "df_time_score = pd.DataFrame(list(score_dct_time.items()))\n",
    "df_time_score.rename(columns = {0: 'variable', 1: 'score'}, inplace = True)\n",
    "df_time_score = df_time_score.sort_values(by = ['score'], ascending = False).reset_index(drop = True)\n",
    "df_time_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the score for each variable for event prediction\n",
    "skb_event = SelectKBest(score_func = f_classif)\n",
    "skb_event.fit_transform(X_train_processed, y_train_2)\n",
    "score_dct_event = dict(zip(X_train_processed.columns.tolist(), skb_event.scores_.round(decimals = 1).tolist()))\n",
    "df_event_score = pd.DataFrame(list(score_dct_event.items()))\n",
    "df_event_score.rename(columns = {0: 'variable', 1: 'score'}, inplace = True)\n",
    "df_event_score = df_event_score.sort_values(by = ['score'], ascending = False).reset_index(drop = True)\n",
    "df_event_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff94ffe1",
   "metadata": {},
   "source": [
    "## Locating outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a77232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outlier on both training and validation data\n",
    "df_all = pd.concat([df_train, df_val])\n",
    "df_all = df_all.sort_values(by = ['case:concept:name', 'time:timestamp']).reset_index(drop = True)\n",
    "\n",
    "def find_outlier(process_name, df):\n",
    "    # Remove outlier having time_diff larger than mean +- 3 * SD\n",
    "    df_needed = df[(df['concept:name'] == process_name)]\n",
    "    mean_value = df_needed['time_diff'].mean()\n",
    "    std_value = df_needed['time_diff'].std()\n",
    "    upper_bound =  mean_value + 3 * std_value\n",
    "    lower_bound = mean_value - 3 * std_value\n",
    "    new_df = df_needed[(df_needed['time_diff'] < lower_bound) | (df_needed['time_diff'] > upper_bound)]\n",
    "    # Return case id that has at least 1 process as outlier\n",
    "    return new_df['case:concept:name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64add03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_lst = []\n",
    "# i refers to the position number\n",
    "for i in tqdm(range(2, len(df_all['position'].tolist()))):\n",
    "    df_pos = df_all[df_all['position'] == i]\n",
    "    # a refers to the concept name per position number\n",
    "    for a in df_pos['concept:name'].unique().tolist():\n",
    "        small_outlier_lst = find_outlier(a, df_pos)\n",
    "        outlier_lst = list(set(outlier_lst + small_outlier_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all outliers\n",
    "df_filtered = df_all[~df_all['case:concept:name'].isin(outlier_lst)]\n",
    "final_all_train = sorted(df_filtered['case:concept:name'].unique().tolist())\n",
    "\n",
    "# Split training and validation dataset\n",
    "final_train, final_val = train_test_split(final_all_train, test_size = 0.2)\n",
    "df_train = df_filtered[df_filtered['case:concept:name'].isin(final_train)]\n",
    "df_val = df_filtered[df_filtered['case:concept:name'].isin(final_val)]\n",
    "\n",
    "# To make sure, again sort the datasets on case and consequently timestamp, then reset the index\n",
    "df_train = df_train.sort_values(by = ['case:concept:name', 'time:timestamp']).reset_index(drop = True)\n",
    "df_val = df_val.sort_values(by = ['case:concept:name', 'time:timestamp']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff0923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7720ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8dc34",
   "metadata": {},
   "source": [
    "## PCA Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce33dbc",
   "metadata": {},
   "source": [
    "### Select data for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a1c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = df_test\n",
    "\n",
    "# Select a subset of features you want \n",
    "features = ['case:RequestedAmount', 'EventOrigin_is_Application', 'EventOrigin_is_Offer', \n",
    "            'EventOrigin_is_Workflow', 'action_is_Created', 'action_is_Deleted',\n",
    "            'action_is_Obtained', 'action_is_Released', 'action_is_statechange',\n",
    "            'lifecycle:transition_is_ate_abort', 'lifecycle:transition_is_complete',\n",
    "            'lifecycle:transition_is_resume', 'lifecycle:transition_is_schedule',\n",
    "            'lifecycle:transition_is_start', 'lifecycle:transition_is_suspend',\n",
    "            'lifecycle:transition_is_withdraw', 'position', 'day_of_month', 'month_no',\n",
    "             'quarter', 'week', 'hour', 'second', 'norm_next_activity_delta', 'norm_past_activity_delta']\n",
    "\n",
    "x = pca_df.loc[:, features].values\n",
    "\n",
    "y = pca_df.loc[:, ['concept:name']].values\n",
    "\n",
    "# Standardize the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f859f",
   "metadata": {},
   "source": [
    "### Perform analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "\n",
    "principal_components = pca.fit_transform(x)\n",
    "\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "\n",
    "# Combine target variable with pricipal components\n",
    "\n",
    "pca_res = pd.concat([principal_df, pca_df[['concept:name']]], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bf8709",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae2b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,15))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "ax.set_title('Dual component PCA Visualization')\n",
    "\n",
    "# Select only the ones you're interested in\n",
    "targets = ['A_Create Application', 'A_Submitted', 'W_Handle leads',\n",
    "       'W_Complete application', 'A_Concept', 'A_Accepted',\n",
    "       'O_Create Offer', 'O_Created', 'O_Sent (mail and online)',\n",
    "       'W_Call after offers', 'A_Complete', 'A_Cancelled', 'O_Cancelled',\n",
    "       'W_Validate application', 'A_Validating', 'O_Returned',\n",
    "       'W_Call incomplete files', 'A_Incomplete', 'O_Accepted',\n",
    "       'A_Pending', 'A_Denied', 'O_Refused', 'O_Sent (online only)',\n",
    "       'W_Assess potential fraud']\n",
    "       \n",
    "colors_keys = [(k) for (k,v) in matplotlib.colors.cnames.items()]\n",
    "colors = random.choices(colors_keys, k=len(features))\n",
    "\n",
    "for target, color in zip(targets, colors):\n",
    "    indicesToKeep = pca_res['concept:name'] == target\n",
    "    ax.scatter(pca_res.loc[indicesToKeep, 'principal component 1']\n",
    "               , pca_res.loc[indicesToKeep, 'principal component 2']\n",
    "               , pca_res.loc[indicesToKeep, 'principal component 3']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "\n",
    "ax.legend(targets)\n",
    "# Change the view\n",
    "# ax.view_init(10, 50)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452a032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5cc74",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e966850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('bci2017_train_filtered.csv', index=False)\n",
    "df_test.to_csv(\"bci2017_test_filtered.csv\", index=False)\n",
    "df_val.to_csv(\"bci2017_val_filtered.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac8fb13fc3bee92f824433c9c23003f97fe6b9b9d15b38f1d3099836166493d1"
  },
  "kernelspec": {
   "display_name": "Python (process-mining)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
