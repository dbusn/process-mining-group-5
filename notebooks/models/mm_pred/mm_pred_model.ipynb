{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNsk1F-54XXA"
   },
   "source": [
    "# Some Notes: \n",
    "\n",
    "\n",
    "*   The 4-tuple output of the model looks like this: (expected, predicted, prob of predicted, prob of expected). When expected == predicted a correct prediction was made.\n",
    "\n",
    "*   For the renaming: Use ProM to convert the .xes file to .csv, by using ProM the case and event attributes are automatically resolved. The values for the case should start with Application, while the values for the event should start with a capital letter and an underscore. The resource column is manually named role instead. \n",
    "\n",
    "*   The k parameter is the size for the prefix to use (the amount of historical events that have to be used for predicting the next event. Padding is used when not enough events are present, should be the same behaviour as in other prediction papers). For selecting the k value: you have to test some settings. Depends on the data, and often also on the model. In this case, it was used to speed up training, and to allow running an update with only very limited number of training data.\n",
    "\n",
    "* On the other hand, a suffix is the remaining sequence of events. \n",
    "\n",
    "*   The split_case parameter allows for cases to be split between train and test set. When set to False, cases will be kept together. This can however result in the unwanted result that some of the events in the test set occured before some events in the train set.\n",
    "\n",
    "*   In the MM-Pred paper, we are absolutely not able to recreate their noted accuracies in Table 2. It is very likely that they used different attributes/features (but they failed to mention which ones, which could require some additional programming to incorporate these attributes/features in the model).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dtlw1FQyEPNg"
   },
   "source": [
    "# Main breakdown of scripts (Adaptor, Model, Modulator)\n",
    "- **Adapter**: Adapter class to allow it to work with the Method class\n",
    "- **Model**: Contains the model itself (there is a version in there where I was experimenting with using CUDNN, so there might be some duplicate code (the basics of this code was taken from the implementation of Camargo et al. so there should be lots of similarities in the way I used some functions etc.\n",
    "- **Modulator**: contains the implementation of the custom layer, refer to the Tensorflow documentation for the meaning of all these functions etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yo4K_cH3fjt"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVbkhfU2uyHn"
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import os\n",
    "from functools import partial\n",
    "import jellyfish as jf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow import multiply, sigmoid, concat, transpose, matmul\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "from dateutil.parser import parse\n",
    "import math\n",
    "import random\n",
    "from operator import itemgetter\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import scipy\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGtayMi_vl_G"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyQ-qVGqvle8"
   },
   "outputs": [],
   "source": [
    "class LogFile:\n",
    "\n",
    "    def __init__(self, filename, delim, header, rows, time_attr, trace_attr, activity_attr = None, values = None, integer_input = False, convert = True, k = 1, dtype=None):\n",
    "        self.filename = filename\n",
    "        self.time = time_attr\n",
    "        self.trace = trace_attr\n",
    "        self.activity = activity_attr\n",
    "        if values is not None:\n",
    "            self.values = values\n",
    "        else:\n",
    "            self.values = {}\n",
    "        self.numericalAttributes = set()\n",
    "        self.categoricalAttributes = set()\n",
    "        self.ignoreHistoryAttributes = set()\n",
    "        if self.trace is None:\n",
    "            self.k = 0\n",
    "        else:\n",
    "            self.k = k\n",
    "\n",
    "        type = \"str\"\n",
    "        if integer_input:\n",
    "            type = \"int\"\n",
    "        if filename is not None:\n",
    "            if dtype is not None:\n",
    "                self.data = pd.read_csv(self.filename, header=header, nrows=rows, delimiter=delim, encoding='latin-1', dtype=dtype)\n",
    "            else:\n",
    "                self.data = pd.read_csv(self.filename, header=header, nrows=rows, delimiter=delim, encoding='latin-1')\n",
    "\n",
    "            # Determine types for all columns - numerical or categorical\n",
    "            for col_type in self.data.dtypes.iteritems():\n",
    "                if col_type[1] == 'float64':\n",
    "                   self.numericalAttributes.add(col_type[0])\n",
    "                else:\n",
    "                    self.categoricalAttributes.add(col_type[0])\n",
    "\n",
    "            if convert:\n",
    "                self.convert2int()\n",
    "\n",
    "            self.contextdata = None\n",
    "\n",
    "    def get_data(self):\n",
    "        if self.contextdata is None:\n",
    "            return self.data\n",
    "        return self.contextdata\n",
    "\n",
    "    def get_cases(self):\n",
    "        return self.get_data().groupby([self.trace])\n",
    "    \n",
    "    def filter_case_length(self, min_length):\n",
    "        cases = self.data.groupby([self.trace])\n",
    "        filtered_cases = []\n",
    "        for case in cases:\n",
    "            if len(case[1]) > min_length:\n",
    "                filtered_cases.append(case[1])\n",
    "        self.data = pd.concat(filtered_cases, ignore_index=True)\n",
    "\n",
    "    def convert2int(self):\n",
    "        self.convert2ints(\"../converted_ints.csv\")\n",
    "\n",
    "    def convert2ints(self, file_out):\n",
    "        \"\"\"\n",
    "        Convert csv file with string values to csv file with integer values.\n",
    "        (File/string operations more efficient than pandas operations)\n",
    "        :param file_out: filename for newly created file\n",
    "        :return: number of lines converted\n",
    "        \"\"\"\n",
    "        self.data = self.data.apply(lambda x: self.convert_column2ints(x))\n",
    "        self.data.to_csv(file_out, index=False)\n",
    "\n",
    "    def convert_column2ints(self, x):\n",
    "\n",
    "        def test(a, b):\n",
    "            # Return all elements from a that are not in b, make use of the fact that both a and b are unique and sorted\n",
    "            a_ix = 0\n",
    "            b_ix = 0\n",
    "            new_uniques = []\n",
    "            while a_ix < len(a) and b_ix < len(b):\n",
    "                if a[a_ix] < b[b_ix]:\n",
    "                    new_uniques.append(a[a_ix])\n",
    "                    a_ix += 1\n",
    "                elif a[a_ix] > b[b_ix]:\n",
    "                    b_ix += 1\n",
    "                else:\n",
    "                    a_ix += 1\n",
    "                    b_ix += 1\n",
    "            if a_ix < len(a):\n",
    "                new_uniques.extend(a[a_ix:])\n",
    "            return new_uniques\n",
    "\n",
    "        if self.isNumericAttribute(x.name):\n",
    "            return x\n",
    "\n",
    "        if self.time is not None and x.name == self.time:\n",
    "            return x\n",
    "\n",
    "        print(\"PREPROCESSING: Converting\", x.name)\n",
    "        if x.name not in self.values:\n",
    "            x = x.astype(\"str\")\n",
    "            self.values[x.name], y = np.unique(x, return_inverse=True)\n",
    "            return y + 1\n",
    "        else:\n",
    "            x = x.astype(\"str\")\n",
    "            self.values[x.name] = np.append(self.values[x.name], test(np.unique(x), self.values[x.name]))\n",
    "\n",
    "            print(\"PREPROCESSING: Substituting values with ints\")\n",
    "            xsorted = np.argsort(self.values[x.name])\n",
    "            ypos = np.searchsorted(self.values[x.name][xsorted], x)\n",
    "            indices = xsorted[ypos]\n",
    "\n",
    "        return indices + 1\n",
    "\n",
    "    def convert_string2int(self, column, value):\n",
    "        if column not in self.values:\n",
    "            return value\n",
    "        vals = self.values[column]\n",
    "        found = np.where(vals==value)\n",
    "        if len(found[0]) == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return found[0][0] + 1\n",
    "\n",
    "    def convert_int2string(self, column, int_val):\n",
    "        if column not in self.values:\n",
    "            return int_val\n",
    "        return self.values[column][int_val - 1]\n",
    "\n",
    "\n",
    "    def attributes(self):\n",
    "        return self.data.columns\n",
    "\n",
    "    def keep_attributes(self, keep_attrs):\n",
    "        if self.time and self.time not in keep_attrs and self.time in self.data:\n",
    "            keep_attrs.append(self.time)\n",
    "        if self.trace and self.trace not in keep_attrs:\n",
    "            keep_attrs.append(self.trace)\n",
    "        self.data = self.data[keep_attrs]\n",
    "\n",
    "    def remove_attributes(self, remove_attrs):\n",
    "        \"\"\"\n",
    "        Remove attributes with the given prefixes from the data\n",
    "        :param remove_attrs: a list of prefixes of attributes that should be removed from the data\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        remove = []\n",
    "        for attr in self.data:\n",
    "            for prefix in remove_attrs:\n",
    "                if attr.startswith(prefix):\n",
    "                    remove.append(attr)\n",
    "                    break\n",
    "\n",
    "        self.data = self.data.drop(remove, axis=1)\n",
    "\n",
    "    def filter(self, filter_condition):\n",
    "        self.data = self.data[eval(filter_condition)]\n",
    "\n",
    "    def filter_copy(self, filter_condition):\n",
    "        log_copy = copy.deepcopy(self)\n",
    "        log_copy.data = self.data[eval(filter_condition)]\n",
    "        return log_copy\n",
    "\n",
    "    def get_column(self, attribute):\n",
    "        return self.data[attribute]\n",
    "\n",
    "    def get_labels(self, label):\n",
    "        labels = {}\n",
    "        if self.trace is None:\n",
    "            for row in self.data.itertuples():\n",
    "                labels[row.Index] = getattr(row, label)\n",
    "        else:\n",
    "            traces = self.data.groupby([self.trace])\n",
    "            for trace in traces:\n",
    "                labels[trace[0]] = getattr(trace[1].iloc[0], label)\n",
    "        return labels\n",
    "\n",
    "    def create_trace_attribute(self):\n",
    "        print(\"Create trace attribute\")\n",
    "        with mp.Pool(mp.cpu_count()) as p:\n",
    "            result = p.map(self.create_trace_attribute_case, self.data.groupby([self.trace]))\n",
    "        self.data = pd.concat(result)\n",
    "        self.categoricalAttributes.add(\"trace\")\n",
    "\n",
    "    def create_trace_attribute_case(self, case_tuple):\n",
    "        trace = []\n",
    "        case_data = pd.DataFrame()\n",
    "        for row in case_tuple[1].iterrows():\n",
    "            row_content = row[1]\n",
    "            trace.append(row_content[self.activity])\n",
    "            row_content[\"trace\"] = str(trace)\n",
    "            case_data = case_data.append(row_content)\n",
    "        return case_data\n",
    "\n",
    "    def create_k_context(self):\n",
    "        \"\"\"\n",
    "        Create the k-context from the current LogFile\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        print(\"Create k-context:\", self.k)\n",
    "\n",
    "        if self.k == 0:\n",
    "            self.contextdata = self.data\n",
    "\n",
    "        if self.contextdata is None:\n",
    "            # result = map(self.create_k_context_trace, self.data.groupby([self.trace]))\n",
    "\n",
    "            with mp.Pool(mp.cpu_count()) as p:\n",
    "                result = p.map(self.create_k_context_trace, self.data.groupby([self.trace]))\n",
    "\n",
    "            # result = map(self.create_k_context_trace, self.data.groupby([self.trace]))\n",
    "\n",
    "            self.contextdata = pd.concat(result, ignore_index=True)\n",
    "\n",
    "    def create_k_context_trace(self, trace):\n",
    "        contextdata = pd.DataFrame()\n",
    "\n",
    "        trace_data = trace[1]\n",
    "        shift_data = trace_data.shift().fillna(0)\n",
    "        shift_data.at[shift_data.first_valid_index(), self.trace] = trace[0]\n",
    "        joined_trace = shift_data.join(trace_data, lsuffix=\"_Prev0\")\n",
    "        for i in range(1, self.k):\n",
    "            shift_data = shift_data.shift().fillna(0)\n",
    "            shift_data.at[shift_data.first_valid_index(), self.trace] = trace[0]\n",
    "            joined_trace = shift_data.join(joined_trace, lsuffix=\"_Prev%i\" % i)\n",
    "        contextdata = contextdata.append(joined_trace, ignore_index=True)\n",
    "        contextdata = contextdata.astype(\"int\", errors=\"ignore\")\n",
    "        return contextdata\n",
    "\n",
    "    def add_duration_to_k_context(self):\n",
    "        \"\"\"\n",
    "        Add durations to the k-context, only calculates if k-context has been calculated\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.contextdata is None:\n",
    "            return\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.contextdata['duration_%i' %(i)] = self.contextdata.apply(self.calc_duration, axis=1, args=(i,))\n",
    "            self.numericalAttributes.add(\"duration_%i\" % (i))\n",
    "\n",
    "    def calc_duration(self, row, k):\n",
    "        if row[self.time + \"_Prev%i\" % (k)] != 0:\n",
    "            startTime = parse(self.convert_int2string(self.time, int(row[self.time + \"_Prev%i\" % (k)])))\n",
    "            endTime = parse(self.convert_int2string(self.time,int(row[self.time])))\n",
    "            return (endTime - startTime).total_seconds()\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def discretize(self,row, bins=25):\n",
    "        if isinstance(bins, int):\n",
    "            labels = [str(i) for i in range(1,bins+1)]\n",
    "        else:\n",
    "            labels = [str(i) for i in range(1,len(bins))]\n",
    "        if self.isNumericAttribute(row):\n",
    "            self.numericalAttributes.remove(row)\n",
    "            self.categoricalAttributes.add(row)\n",
    "            self.contextdata[row], binned = pd.cut(self.contextdata[row], bins, retbins=True, labels=labels)\n",
    "            #self.contextdata[row] = self.contextdata[row].astype(str)\n",
    "            #self.contextdata[row] = self.convert_column2ints(self.contextdata[row])\n",
    "        return binned\n",
    "\n",
    "    def isNumericAttribute(self, attribute):\n",
    "        if attribute in self.numericalAttributes:\n",
    "            return True\n",
    "        else:\n",
    "            for k in range(self.k):\n",
    "                if attribute.replace(\"_Prev%i\" % (k), \"\") in self.numericalAttributes:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def isCategoricalAttribute(self, attribute):\n",
    "        if attribute in self.categoricalAttributes:\n",
    "            return True\n",
    "        else:\n",
    "            for k in range(self.k):\n",
    "                if attribute.replace(\"_Prev%i\" % (k), \"\") in self.categoricalAttributes:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def add_end_events(self):\n",
    "        cases = self.get_cases()\n",
    "        print(\"Run end event map\")\n",
    "        with mp.Pool(mp.cpu_count()) as p:\n",
    "            result = p.map(self.add_end_event_case, cases)\n",
    "\n",
    "        print(\"Combine results\")\n",
    "        new_data = []\n",
    "        for r in result:\n",
    "            new_data.extend(r)\n",
    "\n",
    "        self.data = pd.DataFrame.from_records(new_data)\n",
    "\n",
    "    def add_end_event_case(self, case_obj):\n",
    "        case_name, case = case_obj\n",
    "        new_data = []\n",
    "        for i in range(0, len(case)):\n",
    "            new_data.append(case.iloc[i].to_dict())\n",
    "\n",
    "        record = {}\n",
    "        for col in self.data:\n",
    "            if col == self.trace:\n",
    "                record[col] = case_name\n",
    "            elif col == self.time:\n",
    "                record[col] = new_data[-1][self.time]\n",
    "            else:\n",
    "                record[col] = \"end\"\n",
    "        new_data.append(record)\n",
    "        return new_data\n",
    "\n",
    "    def splitTrainTest(self, train_percentage, split_case=True, method=\"train-test\"):\n",
    "        import random\n",
    "        train_percentage = train_percentage / 100.0\n",
    "\n",
    "        if split_case:\n",
    "            if method == \"random\":\n",
    "                train_inds = random.sample(range(self.contextdata.shape[0]), k=round(self.contextdata.shape[0] * train_percentage))\n",
    "                test_inds = list(set(range(self.contextdata.shape[0])).difference(set(train_inds)))\n",
    "            elif method == \"train-test\":\n",
    "                train_inds = np.arange(0, self.contextdata.shape[0] * train_percentage)\n",
    "                test_inds = list(set(range(self.contextdata.shape[0])).difference(set(train_inds)))\n",
    "            else:\n",
    "                test_inds = np.arange(0, self.contextdata.shape[0] * (1 - train_percentage))\n",
    "                train_inds = list(set(range(self.contextdata.shape[0])).difference(set(test_inds)))\n",
    "        else:\n",
    "            train_inds = []\n",
    "            test_inds = []\n",
    "            cases = self.contextdata[self.trace].unique()\n",
    "            if method == \"random\":\n",
    "                train_cases = random.sample(list(cases), k=round(len(cases) * train_percentage))\n",
    "                test_cases = list(set(cases).difference(set(train_cases)))\n",
    "            elif method == \"train-test\":\n",
    "                train_cases = cases[:round(len(cases) * train_percentage)]\n",
    "                test_cases = cases[round(len(cases) * train_percentage):]\n",
    "            else:\n",
    "                train_cases = cases[round(len(cases) * (1 - train_percentage)):]\n",
    "                test_cases = cases[:round(len(cases) * (1 - train_percentage))]\n",
    "\n",
    "            for train_case in train_cases:\n",
    "                train_inds.extend(list(self.contextdata[self.contextdata[self.trace] == train_case].index))\n",
    "            for test_case in test_cases:\n",
    "                test_inds.extend(list(self.contextdata[self.contextdata[self.trace] == test_case].index))\n",
    "\n",
    "        train = self.contextdata.loc[train_inds]\n",
    "        test = self.contextdata.loc[test_inds]\n",
    "\n",
    "        print(\"Train:\", len(train_inds))\n",
    "        print(\"Test:\", len(test_inds))\n",
    "\n",
    "        train_logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "        train_logfile.filename = self.filename\n",
    "        train_logfile.values = self.values\n",
    "        train_logfile.contextdata = train\n",
    "        train_logfile.categoricalAttributes = self.categoricalAttributes\n",
    "        train_logfile.numericalAttributes = self.numericalAttributes\n",
    "        train_logfile.data = self.data.loc[train_inds]\n",
    "        train_logfile.k = self.k\n",
    "\n",
    "        test_logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "        test_logfile.filename = self.filename\n",
    "        test_logfile.values = self.values\n",
    "        test_logfile.contextdata = test\n",
    "        test_logfile.categoricalAttributes = self.categoricalAttributes\n",
    "        test_logfile.numericalAttributes = self.numericalAttributes\n",
    "        test_logfile.data = self.data.loc[test_inds]\n",
    "        test_logfile.k = self.k\n",
    "\n",
    "        return train_logfile, test_logfile\n",
    "\n",
    "\n",
    "    def split_days(self, date_format, num_days=1):\n",
    "        from datetime import datetime\n",
    "\n",
    "        self.contextdata[\"days\"] = self.contextdata[self.time].map(lambda l: str(datetime.strptime(l, date_format).isocalendar()[:3]))\n",
    "        days = {}\n",
    "        for group_name, group in self.contextdata.groupby(\"days\"):\n",
    "            new_logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "            new_logfile.filename = self.filename\n",
    "            new_logfile.values = self.values\n",
    "            new_logfile.categoricalAttributes = self.categoricalAttributes\n",
    "            new_logfile.numericalAttributes = self.numericalAttributes\n",
    "            new_logfile.k = self.k\n",
    "            new_logfile.contextdata = group.drop(\"days\", axis=1)\n",
    "            new_logfile.data = new_logfile.contextdata[self.attributes()]\n",
    "\n",
    "            days[group_name] = {}\n",
    "            days[group_name][\"data\"] = new_logfile\n",
    "        return days\n",
    "\n",
    "    def split_weeks(self, date_format, num_days=1):\n",
    "        from datetime import datetime\n",
    "\n",
    "        self.contextdata[\"year_week\"] = self.contextdata[self.time].map(lambda l: str(datetime.strptime(l, date_format).isocalendar()[:2]))\n",
    "        weeks = {}\n",
    "        for group_name, group in self.contextdata.groupby(\"year_week\"):\n",
    "            new_logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "            new_logfile.filename = self.filename\n",
    "            new_logfile.values = self.values\n",
    "            new_logfile.categoricalAttributes = self.categoricalAttributes\n",
    "            new_logfile.numericalAttributes = self.numericalAttributes\n",
    "            new_logfile.k = self.k\n",
    "            new_logfile.contextdata = group.drop(\"year_week\", axis=1)\n",
    "            new_logfile.data = new_logfile.contextdata[self.attributes()]\n",
    "\n",
    "            year, week = eval(group_name)\n",
    "            group_name = \"%i/\" % year\n",
    "            if week < 10:\n",
    "                group_name += \"0\"\n",
    "            group_name += str(week)\n",
    "\n",
    "            weeks[group_name] = {}\n",
    "            weeks[group_name][\"data\"] = new_logfile\n",
    "        return weeks\n",
    "\n",
    "    def split_months(self, date_format, num_days=1):\n",
    "        from datetime import datetime\n",
    "        self.contextdata[\"month\"] = self.contextdata[self.time].map(lambda l: str(datetime.strptime(l, date_format).strftime(\"%Y/%m\")))\n",
    "\n",
    "        months = {}\n",
    "        for group_name, group in self.contextdata.groupby(\"month\"):\n",
    "            new_logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "            new_logfile.filename = self.filename\n",
    "            new_logfile.values = self.values\n",
    "            new_logfile.categoricalAttributes = self.categoricalAttributes\n",
    "            new_logfile.numericalAttributes = self.numericalAttributes\n",
    "            new_logfile.k = self.k\n",
    "            new_logfile.contextdata = group.drop(\"month\", axis=1)\n",
    "            new_logfile.data = new_logfile.contextdata[self.attributes()]\n",
    "\n",
    "            months[group_name] = {}\n",
    "            months[group_name][\"data\"] = new_logfile\n",
    "        return months\n",
    "\n",
    "    def split_date(self, date_format, year_week, from_week=None):\n",
    "        from datetime import datetime\n",
    "\n",
    "        self.contextdata[\"year_week\"] = self.contextdata[self.time].map(lambda l: str(datetime.strptime(l, date_format).isocalendar()[:2]))\n",
    "\n",
    "        if from_week:\n",
    "            train = self.contextdata[(self.contextdata[\"year_week\"] >= from_week) & (self.contextdata[\"year_week\"] < year_week)]\n",
    "        else:\n",
    "            train = self.contextdata[self.contextdata[\"year_week\"] < year_week]\n",
    "        test = self.contextdata[self.contextdata[\"year_week\"] == year_week]\n",
    "\n",
    "        train_logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "        train_logfile.filename = self.filename\n",
    "        train_logfile.values = self.values\n",
    "        train_logfile.contextdata = train\n",
    "        train_logfile.categoricalAttributes = self.categoricalAttributes\n",
    "        train_logfile.numericalAttributes = self.numericalAttributes\n",
    "        train_logfile.data = train[self.attributes()]\n",
    "        train_logfile.k = self.k\n",
    "\n",
    "        test_logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "        test_logfile.filename = self.filename\n",
    "        test_logfile.values = self.values\n",
    "        test_logfile.contextdata = test\n",
    "        test_logfile.categoricalAttributes = self.categoricalAttributes\n",
    "        test_logfile.numericalAttributes = self.numericalAttributes\n",
    "        test_logfile.data = test[self.attributes()]\n",
    "        test_logfile.k = self.k\n",
    "\n",
    "        return train_logfile, test_logfile\n",
    "\n",
    "\n",
    "    def create_folds(self, k):\n",
    "        result = []\n",
    "        folds = np.array_split(np.arange(0, self.contextdata.shape[0]), k)\n",
    "        for f in folds:\n",
    "            fold_context = self.contextdata.loc[f]\n",
    "\n",
    "            logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "            logfile.filename = self.filename\n",
    "            logfile.values = self.values\n",
    "            logfile.contextdata = fold_context\n",
    "            logfile.categoricalAttributes = self.categoricalAttributes\n",
    "            logfile.numericalAttributes = self.numericalAttributes\n",
    "            logfile.data = self.data.loc[f]\n",
    "            logfile.k = self.k\n",
    "            result.append(logfile)\n",
    "        return result\n",
    "\n",
    "    def extend_data(self, log):\n",
    "        train_logfile = LogFile(None, None, None, None, self.time, self.trace, self.activity, self.values, False, False)\n",
    "        train_logfile.filename = self.filename\n",
    "        train_logfile.values = self.values\n",
    "        train_logfile.contextdata = self.contextdata.append(log.contextdata)\n",
    "        train_logfile.categoricalAttributes = self.categoricalAttributes\n",
    "        train_logfile.numericalAttributes = self.numericalAttributes\n",
    "        train_logfile.data = self.data.append(log.data)\n",
    "        train_logfile.k = self.k\n",
    "        return train_logfile\n",
    "\n",
    "    def get_traces(self):\n",
    "        return [list(case[1][self.activity]) for case in self.get_cases()]\n",
    "\n",
    "    def get_follows_relations(self, window=None):\n",
    "        return self.get_traces_follows_relations(self.get_traces(), window)\n",
    "\n",
    "    def get_traces_follows_relations(self, traces, window):\n",
    "        follow_counts = {}\n",
    "        counts = {}\n",
    "        for trace in traces:\n",
    "            for i in range(len(trace)):\n",
    "                act = trace[i]\n",
    "                if act not in follow_counts:\n",
    "                    follow_counts[act] = {}\n",
    "                    counts[act] = 0\n",
    "                counts[act] += 1\n",
    "\n",
    "                stop_value = len(trace)\n",
    "                if window:\n",
    "                    stop_value = min(len(trace), i+window)\n",
    "\n",
    "                for fol_act in set(trace[i+1:stop_value+1]):\n",
    "                    if fol_act not in follow_counts[act]:\n",
    "                        follow_counts[act][fol_act] = 0\n",
    "                    follow_counts[act][fol_act] += 1\n",
    "\n",
    "\n",
    "        follows = {}\n",
    "        for a in range(1, len(self.values[self.activity])+1):\n",
    "            always = 0\n",
    "            sometimes = 0\n",
    "            if a in follow_counts:\n",
    "                for b in follow_counts[a]:\n",
    "                    if a != b:\n",
    "                        if follow_counts[a][b] == counts[a]:\n",
    "                            always += 1\n",
    "                        else:\n",
    "                            sometimes += 1\n",
    "            never = len(self.values[self.activity]) - always - sometimes\n",
    "            follows[a] = (always, sometimes, never)\n",
    "\n",
    "        return follows, follow_counts\n",
    "\n",
    "\n",
    "    def get_relation_entropy(self):\n",
    "        follows, _ = self.get_follows_relations()\n",
    "        full_entropy = []\n",
    "        for act in range(1, len(self.values[self.activity])+1):\n",
    "            RC = follows[act]\n",
    "            p_a = RC[0] / len(self.values[self.activity])\n",
    "            p_s = RC[1] / len(self.values[self.activity])\n",
    "            p_n = RC[2] / len(self.values[self.activity])\n",
    "            entropy = 0\n",
    "            if p_a != 0:\n",
    "                entropy -= p_a * math.log(p_a)\n",
    "            if p_s != 0:\n",
    "                entropy -= p_s * math.log(p_s)\n",
    "            if p_n != 0:\n",
    "                entropy -= p_n * math.log(p_n)\n",
    "            full_entropy.append(entropy)\n",
    "        return full_entropy\n",
    "\n",
    "\n",
    "    def get_j_measure_trace(self, trace, window):\n",
    "        _, follows = self.get_traces_follows_relations([trace], window)\n",
    "        j_measure = []\n",
    "        value_counts = {}\n",
    "        for e in trace:\n",
    "            if e not in value_counts:\n",
    "                value_counts[e] = 0\n",
    "            value_counts[e] += 1\n",
    "        for act_1 in range(1, len(self.values[self.activity])+1):\n",
    "            for act_2 in range(1, len(self.values[self.activity]) + 1):\n",
    "                num_events = len(trace)\n",
    "                if act_1 in follows and act_2 in follows[act_1]:\n",
    "                    p_aFb = follows[act_1][act_2] / value_counts.get(act_1, 0)\n",
    "                else:\n",
    "                    p_aFb = 0\n",
    "\n",
    "                if act_1 not in value_counts:\n",
    "                    p_a = 0\n",
    "                else:\n",
    "                    p_a = value_counts.get(act_1, 0)/ num_events\n",
    "\n",
    "                if act_2 not in value_counts:\n",
    "                    p_b = 0\n",
    "                else:\n",
    "                    p_b = value_counts.get(act_2, 0) / num_events\n",
    "\n",
    "                j_value = 0\n",
    "                if p_aFb != 0 and p_b != 0:\n",
    "                    j_value += p_aFb * math.log(p_aFb / p_b, 2)\n",
    "\n",
    "                if p_aFb != 1 and p_b != 1:\n",
    "                    j_value += (1-p_aFb) * math.log((1-p_aFb) / (1-p_b), 2)\n",
    "\n",
    "                j_measure.append(p_a * j_value)\n",
    "\n",
    "        return j_measure\n",
    "\n",
    "\n",
    "    def get_j_measure(self, window=5):\n",
    "        traces = self.get_traces()\n",
    "        # return [np.mean(self.get_j_measure_trace(trace, window)) for trace in traces]\n",
    "        return [self.get_j_measure_trace(trace, window) for trace in traces]\n",
    "        # j_measures = np.asarray([self.get_j_measure_trace(trace, window) for trace in traces])\n",
    "        # avg_j_measures = [np.mean(j_measures[:,i]) for i in range(len(j_measures[0]))]\n",
    "        # return avg_j_measures\n",
    "\n",
    "\n",
    "def combine(logfiles):\n",
    "    if len(logfiles) == 0:\n",
    "        return None\n",
    "\n",
    "    log = copy.deepcopy(logfiles[0])\n",
    "    for i in range(1, len(logfiles)):\n",
    "        log = log.extend_data(logfiles[i])\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUWKAxXTvIye"
   },
   "source": [
    "# Modulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "us9gOWkjvBwZ"
   },
   "outputs": [],
   "source": [
    "REPR_DIM = 100\n",
    "\n",
    "class Modulator(Layer):\n",
    "    def __init__(self, attr_idx, num_attrs, time, **kwargs):\n",
    "        self.attr_idx = attr_idx\n",
    "        self.num_attrs = num_attrs  # Number of extra attributes used in the modulator (other than the event)\n",
    "        self.time_step = time\n",
    "\n",
    "        super(Modulator, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"Modulator_W\", shape=(self.num_attrs+1, (self.num_attrs + 2) * REPR_DIM), initializer=\"uniform\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"Modulator_b\", shape=(self.num_attrs + 1, 1), initializer=\"zeros\", trainable=True)\n",
    "\n",
    "        #super(Modulator, self).build(input_shape)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, x):\n",
    "        # split input to different representation vectors\n",
    "        representations = []\n",
    "        for i in range(self.num_attrs + 1):\n",
    "            representations.append(x[:,((i + 1) * self.time_step) - 1,:])\n",
    "\n",
    "        # Calculate z-vector\n",
    "        tmp = []\n",
    "        for elem_product in range(self.num_attrs + 1):\n",
    "            if elem_product != self.attr_idx:\n",
    "                tmp.append(multiply(representations[self.attr_idx],representations[elem_product], name=\"Modulator_repr_mult_\" + str(elem_product)))\n",
    "        for attr_idx in range(self.num_attrs + 1):\n",
    "            tmp.append(representations[attr_idx])\n",
    "        z = concat(tmp, axis=1, name=\"Modulator_concatz\")\n",
    "        # Calculate b-vectors\n",
    "        b = sigmoid(matmul(self.W,transpose(z), name=\"Modulator_matmulb\") + self.b, name=\"Modulator_sigmoid\")\n",
    "\n",
    "        # Use b-vectors to output\n",
    "        tmp = transpose(multiply(b[0,:], transpose(x[:,(self.attr_idx * self.time_step):((self.attr_idx+1) * self.time_step),:])), name=\"Modulator_mult_0\")\n",
    "        for i in range(1, self.num_attrs + 1):\n",
    "             tmp = tmp + transpose(multiply(b[i,:], transpose(x[:,(i * self.time_step):((i+1) * self.time_step),:])), name=\"Modulator_mult_\" + str(i))\n",
    "\n",
    "        return tmp\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.time_step, REPR_DIM)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'attr_idx': self.attr_idx, 'num_attrs': self.num_attrs, 'time': self.time_step}\n",
    "        base_config = super(Modulator, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCDEBiwovPTZ"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsfO7LxfujJa"
   },
   "outputs": [],
   "source": [
    "def create_model_cudnn(vec, vocab_act_size, vocab_role_size, output_folder):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from tensorflow.keras.layers import Input, Embedding, Dropout, Concatenate, LSTM, Dense, BatchNormalization\n",
    "    from tensorflow.keras.models import Model, load_model\n",
    "    from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "    # Create embeddings + Concat\n",
    "    act_input = Input(shape = (vec['prefixes']['x_ac_inp'].shape[1],), name=\"act_input\")\n",
    "    role_input = Input(shape = (vec['prefixes']['x_rl_inp'].shape[1],), name=\"role_input\")\n",
    "\n",
    "    act_embedding = Embedding(vocab_act_size, 100, input_length=vec['prefixes']['x_ac_inp'].shape[1],)(act_input)\n",
    "    act_dropout = Dropout(0.2)(act_embedding)\n",
    "    act_e_lstm_1 = LSTM(32, return_sequences=True)(act_dropout)\n",
    "    act_e_lstm_2 = LSTM(100, return_sequences=True)(act_e_lstm_1)\n",
    "\n",
    "\n",
    "    role_embedding = Embedding(vocab_role_size, 100, input_length=vec['prefixes']['x_rl_inp'].shape[1],)(role_input)\n",
    "    role_dropout = Dropout(0.2)(role_embedding)\n",
    "    role_e_lstm_1 = LSTM(32, return_sequences=True)(role_dropout)\n",
    "    role_e_lstm_2 = LSTM(100, return_sequences=True)(role_e_lstm_1)\n",
    "\n",
    "    concat1 = Concatenate(axis=1)([act_e_lstm_2, role_e_lstm_2])\n",
    "    normal = BatchNormalization()(concat1)\n",
    "\n",
    "    act_modulator = Modulator(attr_idx=0, num_attrs=1)(normal)\n",
    "    role_modulator = Modulator(attr_idx=1, num_attrs=1)(normal)\n",
    "\n",
    "    # Use LSTM to decode events\n",
    "    act_d_lstm_1 = LSTM(100, return_sequences=True)(act_modulator)\n",
    "    act_d_lstm_2 = LSTM(32, return_sequences=False)(act_d_lstm_1)\n",
    "\n",
    "    role_d_lstm_1 = LSTM(100, return_sequences=True)(role_modulator)\n",
    "    role_d_lstm_2 = LSTM(32, return_sequences=False)(role_d_lstm_1)\n",
    "\n",
    "    act_output = Dense(vocab_act_size, name=\"act_output\", activation='softmax')(act_d_lstm_2)\n",
    "    role_output = Dense(vocab_role_size, name=\"role_output\", activation=\"softmax\")(role_d_lstm_2)\n",
    "\n",
    "    model = Model(inputs=[act_input, role_input], outputs=[act_output, role_output])\n",
    "\n",
    "    opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999,\n",
    "                epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "    model.compile(loss={'act_output': 'categorical_crossentropy', 'role_output': 'categorical_crossentropy'}, optimizer=opt)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, 'model_rd_{epoch:03d}-{val_loss:.2f}.h5')\n",
    "\n",
    "    # Saving\n",
    "    model_checkpoint = ModelCheckpoint(output_file_path,\n",
    "                                       monitor='val_loss',\n",
    "                                       verbose=1,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='auto')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
    "\n",
    "    model.fit({'act_input':vec['prefixes']['x_ac_inp'],\n",
    "               'role_input':vec['prefixes']['x_rl_inp']},\n",
    "              {'act_output':vec['next_evt']['y_ac_inp'],\n",
    "               'role_output':vec['next_evt']['y_rl_inp']},\n",
    "              validation_split=0.2,\n",
    "              verbose=2,\n",
    "              batch_size=5,\n",
    "              callbacks=[early_stopping, model_checkpoint],\n",
    "              epochs=200)\n",
    "\n",
    "def create_model(log, output_folder, epochs, early_stop):\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    from tensorflow.keras.layers import Input, Embedding, Dropout, Concatenate, LSTM, Dense, BatchNormalization\n",
    "    from tensorflow.keras.models import Model, load_model\n",
    "    from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "    vec = vectorization(log)\n",
    "    vocab_act_size = len(log.values[\"event\"]) + 1\n",
    "    vocab_role_size = len(log.values[\"role\"]) + 1\n",
    "\n",
    "    # Create embeddings + Concat\n",
    "    act_input = Input(shape=(vec['prefixes']['x_ac_inp'].shape[1],), name=\"act_input\")\n",
    "    role_input = Input(shape=(vec['prefixes']['x_rl_inp'].shape[1],), name=\"role_input\")\n",
    "\n",
    "    act_embedding = Embedding(vocab_act_size, 100, input_length=vec['prefixes']['x_ac_inp'].shape[1],)(act_input)\n",
    "    act_dropout = Dropout(0.2)(act_embedding)\n",
    "    act_e_lstm_1 = LSTM(32, return_sequences=True)(act_dropout)\n",
    "    act_e_lstm_2 = LSTM(100, return_sequences=True)(act_e_lstm_1)\n",
    "\n",
    "\n",
    "    role_embedding = Embedding(vocab_role_size, 100, input_length=vec['prefixes']['x_rl_inp'].shape[1],)(role_input)\n",
    "    role_dropout = Dropout(0.2)(role_embedding)\n",
    "    role_e_lstm_1 = LSTM(32, return_sequences=True)(role_dropout)\n",
    "    role_e_lstm_2 = LSTM(100, return_sequences=True)(role_e_lstm_1)\n",
    "\n",
    "    concat1 = Concatenate(axis=1)([act_e_lstm_2, role_e_lstm_2])\n",
    "    normal = BatchNormalization()(concat1)\n",
    "\n",
    "    act_modulator = Modulator(attr_idx=0, num_attrs=1, time=log.k)(normal)\n",
    "    role_modulator = Modulator(attr_idx=1, num_attrs=1, time=log.k)(normal)\n",
    "\n",
    "    # Use LSTM to decode events\n",
    "    act_d_lstm_1 = LSTM(100, return_sequences=True)(act_modulator)\n",
    "    act_d_lstm_2 = LSTM(32, return_sequences=False)(act_d_lstm_1)\n",
    "\n",
    "    role_d_lstm_1 = LSTM(100, return_sequences=True)(role_modulator)\n",
    "    role_d_lstm_2 = LSTM(32, return_sequences=False)(role_d_lstm_1)\n",
    "\n",
    "    act_output = Dense(vocab_act_size, name=\"act_output\", activation='softmax')(act_d_lstm_2)\n",
    "    role_output = Dense(vocab_role_size, name=\"role_output\", activation=\"softmax\")(role_d_lstm_2)\n",
    "\n",
    "    model = Model(inputs=[act_input, role_input], outputs=[act_output, role_output])\n",
    "\n",
    "    opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999,\n",
    "                epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "    model.compile(loss={'act_output': 'categorical_crossentropy', 'role_output': 'categorical_crossentropy'}, optimizer=opt)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, 'model_{epoch:03d}-{val_loss:.2f}.h5')\n",
    "\n",
    "    # Saving\n",
    "    model_checkpoint = ModelCheckpoint(output_file_path,\n",
    "                                       monitor='val_loss',\n",
    "                                       verbose=1,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=False,\n",
    "                                       mode='auto')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop)\n",
    "\n",
    "    model.fit({'act_input':vec['prefixes']['x_ac_inp'],\n",
    "               'role_input':vec['prefixes']['x_rl_inp']},\n",
    "              {'act_output':vec['next_evt']['y_ac_inp'],\n",
    "               'role_output':vec['next_evt']['y_rl_inp']},\n",
    "              validation_split=0.2,\n",
    "              verbose=2,\n",
    "              batch_size=5,\n",
    "              callbacks=[early_stopping, model_checkpoint],\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_next(log, model):\n",
    "    prefixes = create_pref_next(log)\n",
    "    return _predict_next(model, prefixes)\n",
    "\n",
    "\n",
    "def predict_suffix(model, data):\n",
    "    prefixes = create_pref_suf(data.test_orig)\n",
    "    prefixes = _predict_suffix(model, prefixes, 100, data.logfile.convert_string2int(data.logfile.activity, \"end\"))\n",
    "    prefixes = dl_measure(prefixes)\n",
    "\n",
    "    average_dl = (np.sum([x['suffix_dl'] for x in prefixes]) / len(prefixes))\n",
    "\n",
    "    print(\"Average DL:\", average_dl)\n",
    "    return average_dl\n",
    "\n",
    "\n",
    "def vectorization(log):\n",
    "    \"\"\"Example function with types documented in the docstring.\n",
    "    Args:\n",
    "        log_df (dataframe): event log data.\n",
    "        ac_index (dict): index of activities.\n",
    "        rl_index (dict): index of roles.\n",
    "    Returns:\n",
    "        dict: Dictionary that contains all the LSTM inputs.\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "    print(\"Start Vectorization\")\n",
    "\n",
    "    vec = {'prefixes': dict(), 'next_evt': dict()}\n",
    "\n",
    "    train_cases = log.get_cases()\n",
    "    part_vect_map = partial(vect_map, prefix_size=log.k)\n",
    "    with mp.Pool(mp.cpu_count()) as p:\n",
    "        result = np.array(p.map(part_vect_map, train_cases))\n",
    "\n",
    "    vec['prefixes']['x_ac_inp'] = np.concatenate(result[:, 0])\n",
    "    vec['prefixes']['x_rl_inp'] = np.concatenate(result[:, 1])\n",
    "    vec['next_evt']['y_ac_inp'] = np.concatenate(result[:, 2])\n",
    "    vec['next_evt']['y_rl_inp'] = np.concatenate(result[:, 3])\n",
    "\n",
    "    vec['next_evt']['y_ac_inp'] = to_categorical(vec['next_evt']['y_ac_inp'], num_classes=len(log.values[\"event\"])+1)\n",
    "    vec['next_evt']['y_rl_inp'] = to_categorical(vec['next_evt']['y_rl_inp'], num_classes=len(log.values[\"role\"])+1)\n",
    "    return vec\n",
    "\n",
    "\n",
    "def map_case(x, log_df, case_attr):\n",
    "    return log_df[log_df[case_attr] == x]\n",
    "\n",
    "\n",
    "def vect_map(case, prefix_size):\n",
    "    case_df = case[1]\n",
    "\n",
    "    x_ac_inps = []\n",
    "    x_rl_inps = []\n",
    "    y_ac_inps = []\n",
    "    y_rl_inps = []\n",
    "    for row in case_df.iterrows():\n",
    "        row = row[1]\n",
    "        x_ac_inp = []\n",
    "        x_rl_inp = []\n",
    "        for i in range(prefix_size - 1, 0, -1):\n",
    "            x_ac_inp.append(row[\"event_Prev%i\" % i])\n",
    "            x_rl_inp.append(row[\"role_Prev%i\" % i])\n",
    "        x_ac_inp.append(row[\"event_Prev0\"])\n",
    "        x_rl_inp.append(row[\"role_Prev0\"])\n",
    "\n",
    "        x_ac_inps.append(x_ac_inp)\n",
    "        x_rl_inps.append(x_rl_inp)\n",
    "        y_ac_inps.append(row[\"event\"])\n",
    "        y_rl_inps.append(row[\"role\"])\n",
    "    return [np.array(x_ac_inps), np.array(x_rl_inps), np.array(y_ac_inps), np.array(y_rl_inps)]\n",
    "\n",
    "\n",
    "def create_pref_next(log):\n",
    "    \"\"\"Extraction of prefixes and expected suffixes from event log.\n",
    "    Args:\n",
    "        df_test (dataframe): testing dataframe in pandas format.\n",
    "        case_attr: name of attribute containing case ID\n",
    "        activity_attr: name of attribute containing the activity\n",
    "    Returns:\n",
    "        list: list of prefixes and expected sufixes.\n",
    "    \"\"\"\n",
    "    prefixes = []\n",
    "    print(type(log))\n",
    "    cases = log.get_cases()\n",
    "    for case in cases:\n",
    "        trace = case[1]\n",
    "\n",
    "        for row in trace.iterrows():\n",
    "            row = row[1]\n",
    "            ac_pref = []\n",
    "            rl_pref = []\n",
    "            t_pref = []\n",
    "            for i in range(log.k - 1, -1, -1):\n",
    "                ac_pref.append(row[\"event_Prev%i\" % i])\n",
    "                rl_pref.append(row[\"role_Prev%i\" % i])\n",
    "                t_pref.append(0)\n",
    "            prefixes.append(dict(ac_pref=ac_pref,\n",
    "                                 ac_next=row[\"event\"],\n",
    "                                 rl_pref=rl_pref,\n",
    "                                 rl_next=row[\"role\"],\n",
    "                                 t_pref=t_pref))\n",
    "    return prefixes\n",
    "\n",
    "def create_pref_suf(log):\n",
    "    prefixes = []\n",
    "    cases = log.get_cases()\n",
    "    for case in cases:\n",
    "        trace = case[1]\n",
    "\n",
    "        trace_ac = list(trace[\"event\"])\n",
    "        trace_rl = list(trace[\"role\"])\n",
    "\n",
    "        j = 0\n",
    "        for row in trace.iterrows():\n",
    "            row = row[1]\n",
    "            ac_pref = []\n",
    "            rl_pref = []\n",
    "            t_pref = []\n",
    "            for i in range(log.k - 1, -1, -1):\n",
    "                ac_pref.append(row[\"event_Prev%i\" % i])\n",
    "                rl_pref.append(row[\"role_Prev%i\" % i])\n",
    "                t_pref.append(0)\n",
    "            prefixes.append(dict(ac_pref=ac_pref,\n",
    "                                 ac_suff=[x for x in trace_ac[j + 1:]],\n",
    "                                 rl_pref=rl_pref,\n",
    "                                 rl_suff=[x for x in trace_rl[j + 1:]],\n",
    "                                 t_pref=t_pref))\n",
    "            j += 1\n",
    "    return prefixes\n",
    "\n",
    "def _predict_next(model, prefixes):\n",
    "    from tqdm import tqdm \n",
    "    \"\"\"Generate business process suffixes using a keras trained model.\n",
    "    Args:\n",
    "        model (keras model): keras trained model.\n",
    "        prefixes (list): list of prefixes.\n",
    "    \"\"\"\n",
    "    # Generation of predictions\n",
    "    results = []\n",
    "    for prefix in tqdm(prefixes):\n",
    "        # Activities and roles input shape(1,5)\n",
    "\n",
    "        x_ac_ngram = np.array([prefix['ac_pref']])\n",
    "        x_rl_ngram = np.array([prefix['rl_pref']])\n",
    "\n",
    "        predictions = model.predict([x_ac_ngram, x_rl_ngram])\n",
    "\n",
    "        pos = np.argmax(predictions[0][0])\n",
    "        # print(prefix['ac_next'])\n",
    "        # print(pos)\n",
    "        # print(predictions)\n",
    "\n",
    "        results.append((prefix[\"ac_next\"], pos, predictions[0][0][pos], predictions[0][0][int(prefix[\"ac_next\"])]))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _predict_suffix(model, prefixes, max_trace_size, end):\n",
    "    \"\"\"Generate business process suffixes using a keras trained model.\n",
    "    Args:\n",
    "        model (keras model): keras trained model.\n",
    "        prefixes (list): list of prefixes.\n",
    "        max_trace_size: maximum length of a trace in the log\n",
    "        end: value representing the END token\n",
    "    \"\"\"\n",
    "    # Generation of predictions\n",
    "    for prefix in prefixes:\n",
    "        # Activities and roles input shape(1,5)\n",
    "        x_ac_ngram = np.append(\n",
    "            np.zeros(5),\n",
    "            np.array(prefix['ac_pref']),\n",
    "            axis=0)[-5:].reshape((1, 5))\n",
    "\n",
    "        x_rl_ngram = np.append(\n",
    "            np.zeros(5),\n",
    "            np.array(prefix['rl_pref']),\n",
    "            axis=0)[-5:].reshape((1, 5))\n",
    "\n",
    "        ac_suf, rl_suf = list(), list()\n",
    "        for _ in range(1, max_trace_size):\n",
    "            predictions = model.predict([x_ac_ngram, x_rl_ngram])\n",
    "            pos = np.argmax(predictions[0][0])\n",
    "            pos1 = np.argmax(predictions[1][0])\n",
    "            # Activities accuracy evaluation\n",
    "            x_ac_ngram = np.append(x_ac_ngram, [[pos]], axis=1)\n",
    "            x_ac_ngram = np.delete(x_ac_ngram, 0, 1)\n",
    "\n",
    "            x_rl_ngram = np.append(x_rl_ngram, [[pos1]], axis=1)\n",
    "            x_rl_ngram = np.delete(x_rl_ngram, 0, 1)\n",
    "\n",
    "            # Stop if the next prediction is the end of the trace\n",
    "            # otherwise until the defined max_size\n",
    "            ac_suf.append(pos)\n",
    "            rl_suf.append(pos1)\n",
    "\n",
    "            if pos == end:\n",
    "                break\n",
    "\n",
    "        prefix['suff_pred'] = ac_suf\n",
    "        prefix['rl_suff_pred'] = rl_suf\n",
    "    return prefixes\n",
    "\n",
    "\n",
    "def dl_measure(prefixes):\n",
    "    \"\"\"Demerau-Levinstain distance measurement.\n",
    "    Args:\n",
    "        prefixes (list): list with predicted and expected suffixes.\n",
    "    Returns:\n",
    "        list: list with measures added.\n",
    "    \"\"\"\n",
    "    for prefix in prefixes:\n",
    "        suff_log = str([x for x in prefix['suff']])\n",
    "        suff_pred = str([x for x in prefix['suff_pred']])\n",
    "\n",
    "        length = np.max([len(suff_log), len(suff_pred)])\n",
    "        sim = jf.damerau_levenshtein_distance(suff_log,\n",
    "                                              suff_pred)\n",
    "        sim = (1 - (sim / length))\n",
    "        prefix['suffix_dl'] = sim\n",
    "    return prefixes\n",
    "\n",
    "def train(logfile, train_log, model_folder):\n",
    "    create_model(vectorization(train_log.data, train_log.trace, \"event\", num_classes=len(logfile.values[logfile.activity]) + 1), len(logfile.values[logfile.activity]) + 1, len(logfile.values[\"role\"]) + 1, model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgEbSoLPvegO"
   },
   "source": [
    "# Loading Data, Adaptor, and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDw1RbHPyGeQ"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../bpi_2017.csv')\n",
    "df2 = pd.read_csv('../BPI_Challenge_2012.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pN3SF3mY-os0"
   },
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns = {\"concept:name\": 'event', \"case:concept:name\": 'case', \"org:resource\": 'role'})\n",
    "df1 = df1.drop(columns=['Unnamed: 0'])\n",
    "# df_1_small = df1.head(5000) \n",
    "# df_1_small.to_csv('BPIC2017_SMALL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qb1UzuroG_mb"
   },
   "outputs": [],
   "source": [
    "df2 = df2.rename(columns = {\"concept:name\": 'event', \"case:concept:name\": 'case', \"org:resource\": 'role'})\n",
    "df2 = df2.drop(columns=['Unnamed: 0'])\n",
    "# df_1_small = df1.head(5000) \n",
    "# df_1_small.to_csv('BPIC2017_SMALL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7clI-zl6BSC"
   },
   "outputs": [],
   "source": [
    "df1.to_csv('BPIC2017_FULL.csv', index=False)\n",
    "df2.to_csv('BPIC2012_FULL.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AjFNYXq8vfrg",
    "outputId": "d7b81ad5-e930-452d-bfcd-05e701615bd1"
   },
   "outputs": [],
   "source": [
    "def train(log, epochs=200, early_stop=42):\n",
    "    return create_model(log, \"tmp\", epochs, early_stop)\n",
    "\n",
    "\n",
    "def update(model, log):\n",
    "    vec = vectorization(log)\n",
    "\n",
    "    model.fit({'act_input':vec['prefixes']['x_ac_inp'],\n",
    "               'role_input':vec['prefixes']['x_rl_inp']},\n",
    "              {'act_output':vec['next_evt']['y_ac_inp'],\n",
    "               'role_output':vec['next_evt']['y_rl_inp']},\n",
    "              validation_split=0.2,\n",
    "              verbose=2,\n",
    "              batch_size=5,\n",
    "              epochs=10)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model, log):\n",
    "    #print(type(log), 'In test func')\n",
    "    return predict_next(log, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = \"/content/drive/MyDrive/DBL Process Mining/Data/BPIC12.csv\"\n",
    "data = 'BPIC2017_FULL.csv'\n",
    "case_attr = \"case\"\n",
    "act_attr = \"event\"\n",
    "\n",
    "logfile = LogFile(data, \",\", 0, None, None, case_attr,\n",
    "                  activity_attr=act_attr, convert=False, k=10)\n",
    "logfile.convert2int()\n",
    "logfile.filter_case_length(5)\n",
    "logfile.create_k_context()\n",
    "train_log, test_log = logfile.splitTrainTest(80, split_case=False, method=\"test-train\")\n",
    "\n",
    "model = train(train_log, epochs=100, early_stop=10)\n",
    "\n",
    "acc = test(model, test_log)\n",
    "print(acc)\n",
    "print(len(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCjensxLzhKR"
   },
   "source": [
    "# Final Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PLRw_Wz1ml3",
    "outputId": "00bcc329-fe76-4ce1-88f5-b951cefabcce"
   },
   "outputs": [],
   "source": [
    "# Accuracy \n",
    "sum = 0 \n",
    "total = 0\n",
    "for elem in acc: \n",
    "  if elem[0] == elem[1]:\n",
    "    sum += 1\n",
    "  total += 1\n",
    "\n",
    "f'Accuracy: {sum / total}'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5ndLZwEzf22",
    "outputId": "a691117e-cf81-4890-af02-1fd241fd756e"
   },
   "outputs": [],
   "source": [
    "correct_predicted = {}\n",
    "total_predicted = {}\n",
    "total_value = {}\n",
    "\n",
    "for elem in acc:\n",
    "  expected_val = elem[0]\n",
    "  predicted_val = elem[1]\n",
    "\n",
    "  if predicted_val not in total_predicted:\n",
    "    total_predicted[predicted_val] = 0\n",
    "    total_predicted[predicted_val] += 1\n",
    "\n",
    "  if expected_val not in total_value:\n",
    "    total_value[expected_val] = 0\n",
    "    total_value[expected_val] += 1\n",
    "\n",
    "  if elem[0] == elem[1]:\n",
    "    if predicted_val not in correct_predicted:\n",
    "      correct_predicted[predicted_val] = 0\n",
    "    correct_predicted[predicted_val] += 1\n",
    "\n",
    "sum = 0\n",
    "for val in total_predicted.keys():\n",
    "  sum += total_value.get(val, 0) * (correct_predicted.get(val, 0) / total_predicted[val])\n",
    "\n",
    "f'Precision: {sum / len(acc)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Recall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "orXCgmYt01QQ",
    "outputId": "efc21f99-e098-4d10-9697-02609018a0fd"
   },
   "outputs": [],
   "source": [
    "correct_predicted = {}\n",
    "total_value = {}\n",
    "\n",
    "for elem in acc:\n",
    "  expected_val = elem[0]\n",
    "  predicted_val = elem[1]\n",
    "\n",
    "  if expected_val not in total_value:\n",
    "    total_value[expected_val] = 0\n",
    "  total_value[expected_val] += 1\n",
    "\n",
    "  if elem[0] == elem[1]:\n",
    "    if predicted_val not in correct_predicted:\n",
    "      correct_predicted[predicted_val] = 0\n",
    "    correct_predicted[predicted_val] += 1\n",
    "\n",
    "  sum = 0\n",
    "  for val in total_value.keys():\n",
    "    sum += total_value[val] * (correct_predicted.get(val, 0) / total_value[val])\n",
    "\n",
    "f'Recall: {sum / len(acc)}'"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3yo4K_cH3fjt",
    "GGtayMi_vl_G",
    "sUWKAxXTvIye",
    "GCDEBiwovPTZ"
   ],
   "machine_shape": "hm",
   "name": "mm_pred_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (process-mining)",
   "language": "python",
   "name": "dbl-process-mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}